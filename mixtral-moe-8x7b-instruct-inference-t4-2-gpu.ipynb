{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfcf007d",
   "metadata": {
    "papermill": {
     "duration": 0.007425,
     "end_time": "2024-02-04T07:17:48.643541",
     "exception": false,
     "start_time": "2024-02-04T07:17:48.636116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## About the Model : Mistral Mixture of Experts (MoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acf442",
   "metadata": {
    "papermill": {
     "duration": 0.007313,
     "end_time": "2024-02-04T07:17:48.658133",
     "exception": false,
     "start_time": "2024-02-04T07:17:48.650820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p><img src=\"https://www.unite.ai/wp-content/uploads/2023/12/DALL%C2%B7E-2023-12-11-19.51.47-Design-a-banner-with-a-white-background-intended-to-showcase-pixelated-letters-_M-8x7_-in-a-bold-font-style.-The-letters-should-feature-a-gradient-ef.png\" height=\"700\" width=\"700\" style=\"object-fit: cover;\"></p>\n",
    "<p style=\"text-align:justify;\">\n",
    "</p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3425ec1c",
   "metadata": {
    "papermill": {
     "duration": 0.008406,
     "end_time": "2024-02-04T07:17:48.675047",
     "exception": false,
     "start_time": "2024-02-04T07:17:48.666641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"anchor\" id=\"top\" style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto;\n",
    "    padding: 10px;\n",
    "   font-size : 120%;\n",
    "    background-color: #FEF2EF;\n",
    "    border-radius: 3px;\n",
    "    font-color :  #581845  ;        \n",
    "    border: 1px solid #FF5733 ;\">  \n",
    "    \n",
    "Mixtral 8x7B, often referred to as a \"miniature GPT-4,\" leverages a Mixture of Experts (MoE) architecture with eight individual experts. Each expert possesses 111 billion parameters, and when combined with the 55 billion shared attention parameters, the total parameter count per model reaches 166 billion. This architectural decision is noteworthy because it allows only two experts to participate in the inference process for each token, signifying a move towards more streamlined and targeted AI processing.\n",
    "\n",
    "A standout feature of Mixtral is its capacity to handle an extensive context of 32,000 tokens, offering a wide-ranging scope for tackling intricate tasks. Moreover, the model's multilingual support extends to English, French, Italian, German, and Spanish, making it a versatile tool for a diverse global developer community.\n",
    "    \n",
    "Read more about it here : https://www.unite.ai/mistral-ais-latest-mixture-of-experts-moe-8x7b-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5491a1",
   "metadata": {
    "papermill": {
     "duration": 0.008699,
     "end_time": "2024-02-04T07:17:48.692042",
     "exception": false,
     "start_time": "2024-02-04T07:17:48.683343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"anchor\" id=\"top\" style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto;\n",
    "    padding: 10px;\n",
    "   font-size : 120%;\n",
    "    background-color: #FEF2EF;\n",
    "    border-radius: 2px;\n",
    "    font-color :  #581845  ;        \n",
    "    border: 1px solid #FF5733 ;\"> \n",
    "    \n",
    "- Installing the relevant libraries from the github repository itself : [POV : There are methods like e.g. 4-bit peft adapter merge_and_unload() which are still in the beta version, else every functionality which is in the released version behaves the same way]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b780e01",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-04T07:17:48.710554Z",
     "iopub.status.busy": "2024-02-04T07:17:48.709931Z",
     "iopub.status.idle": "2024-02-04T07:19:51.618639Z",
     "shell.execute_reply": "2024-02-04T07:19:51.617358Z"
    },
    "papermill": {
     "duration": 122.920896,
     "end_time": "2024-02-04T07:19:51.621335",
     "exception": false,
     "start_time": "2024-02-04T07:17:48.700439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/huggingface/transformers.git  -U \n",
    "!pip install git+https://github.com/huggingface/accelerate.git  -U \n",
    "!pip install bitsandbytes \n",
    "!pip install git+https://github.com/huggingface/peft.git  -U "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e18e95",
   "metadata": {
    "papermill": {
     "duration": 0.007017,
     "end_time": "2024-02-04T07:19:51.635703",
     "exception": false,
     "start_time": "2024-02-04T07:19:51.628686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d8fd8",
   "metadata": {
    "papermill": {
     "duration": 0.006715,
     "end_time": "2024-02-04T07:19:51.649566",
     "exception": false,
     "start_time": "2024-02-04T07:19:51.642851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"anchor\" id=\"top\" style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto;\n",
    "    padding: 10px;\n",
    "   font-size : 120%;\n",
    "    background-color: #FEF2EF;\n",
    "    border-radius: 2px;\n",
    "    font-color :  #581845  ;        \n",
    "    border: 2px solid #FF5733 ;\"> \n",
    "    \n",
    "- If you're new to quantization,start by reading [this](https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers#:~:text=In%20decimal%2C%20very%20large%20numbers,be%20used%20for%20binary%20numbers.) page.\n",
    "    \n",
    "- The quantization method is based on the paper QLoRA paper whose abstract follows as :\n",
    "    >*We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimizers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training*\n",
    "    \n",
    "- Bitsandbytes makes it easy to quantize and finetune the model with lesser memory requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854632c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T07:19:51.667320Z",
     "iopub.status.busy": "2024-02-04T07:19:51.666982Z",
     "iopub.status.idle": "2024-02-04T07:20:03.002406Z",
     "shell.execute_reply": "2024-02-04T07:20:03.001620Z"
    },
    "papermill": {
     "duration": 11.347718,
     "end_time": "2024-02-04T07:20:03.004739",
     "exception": false,
     "start_time": "2024-02-04T07:19:51.657021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "llm_int8_enable_fp32_cpu_offload= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3a7f8",
   "metadata": {
    "papermill": {
     "duration": 0.006893,
     "end_time": "2024-02-04T07:20:03.019244",
     "exception": false,
     "start_time": "2024-02-04T07:20:03.012351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"anchor\" id=\"top\" style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto;\n",
    "    padding: 10px;\n",
    "   font-size : 120%;\n",
    "    background-color: #FEF2EF;\n",
    "    border-radius: 2px;\n",
    "    font-color :  #581845  ;        \n",
    "    border: 2px solid #FF5733 ;\"> \n",
    "    \n",
    "- `load_in_4bit` : bitsandbytes stores weights in 4-bits\n",
    "- `bnb_4bit_quant_type= \"nf4\"` : normalized float 4 (as per the QLoRA paper,using NF4 quantization is recommended for better performance\n",
    "- `bnb_4bit_compute_dtype= torch.bfloat16` : \n",
    "<p><img src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Three_floating-point_formats.max-700x700.png\" height=\"600\" width=\"600\" style=\"object-fit: cover;\"></p>\n",
    "<p style=\"text-align:justify;\">\n",
    "</p>  \n",
    "The dynamic range of bfloat16 and float32 are equivalent. However, bfloat16 takes up half the memory space\n",
    "- `bnb_4bit_use_double_quant= True` :  Uses a second quantization after the first one to save an additional 0.4 bits per parameter\n",
    "- `llm_int8_enable_fp32_cpu_offload = True` : If you want to split your model in different parts and run some parts in int8 on GPU and some parts in fp32 on CPU, you can use this flag. This is useful for offloading large models such as google/flan-t5-xxl. Note that the int8 operations will not be run on CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a65a568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T07:20:03.034871Z",
     "iopub.status.busy": "2024-02-04T07:20:03.034101Z",
     "iopub.status.idle": "2024-02-04T07:20:03.038268Z",
     "shell.execute_reply": "2024-02-04T07:20:03.037415Z"
    },
    "papermill": {
     "duration": 0.014206,
     "end_time": "2024-02-04T07:20:03.040221",
     "exception": false,
     "start_time": "2024-02-04T07:20:03.026015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc08968",
   "metadata": {
    "papermill": {
     "duration": 0.007186,
     "end_time": "2024-02-04T07:20:03.054163",
     "exception": false,
     "start_time": "2024-02-04T07:20:03.046977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"anchor\" id=\"top\" style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto;\n",
    "    padding: 10px;\n",
    "   font-size : 120%;\n",
    "    background-color: #FEF2EF;\n",
    "    border-radius: 2px;\n",
    "    font-color :  #581845  ;        \n",
    "    border: 2px solid #FF5733 ;\"> \n",
    "    \n",
    "`torch.cuda.empty_cache()` : releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f05824",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T07:20:03.068992Z",
     "iopub.status.busy": "2024-02-04T07:20:03.068458Z",
     "iopub.status.idle": "2024-02-04T07:20:03.201922Z",
     "shell.execute_reply": "2024-02-04T07:20:03.201127Z"
    },
    "papermill": {
     "duration": 0.143405,
     "end_time": "2024-02-04T07:20:03.204352",
     "exception": false,
     "start_time": "2024-02-04T07:20:03.060947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310695c",
   "metadata": {
    "papermill": {
     "duration": 0.00716,
     "end_time": "2024-02-04T07:20:03.219203",
     "exception": false,
     "start_time": "2024-02-04T07:20:03.212043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b5bd81d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T07:20:03.234981Z",
     "iopub.status.busy": "2024-02-04T07:20:03.234303Z",
     "iopub.status.idle": "2024-02-04T07:33:07.951807Z",
     "shell.execute_reply": "2024-02-04T07:33:07.950790Z"
    },
    "papermill": {
     "duration": 784.727887,
     "end_time": "2024-02-04T07:33:07.954507",
     "exception": false,
     "start_time": "2024-02-04T07:20:03.226620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fdf3dc32584d2a8783b0987d56c465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        '/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1',\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54ca3a8",
   "metadata": {
    "papermill": {
     "duration": 0.006956,
     "end_time": "2024-02-04T07:33:07.970897",
     "exception": false,
     "start_time": "2024-02-04T07:33:07.963941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"anchor\" id=\"top\" style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto;\n",
    "    padding: 10px;\n",
    "   font-size : 120%;\n",
    "    background-color: #FEF2EF;\n",
    "    border-radius: 2px;\n",
    "    font-color :  #581845  ;        \n",
    "    border: 2px solid #FF5733 ;\"> \n",
    "    \n",
    "- `device_map=\"auto\"` : pass \"auto\" to get a device map that will be automatically inferred).Manually setting a device once the model has been loaded with device_map is not recommended when using accelerate. So any device assignment call to the model, or to any model’s submodules should be avoided after that line - unless you know what you are doing\n",
    "- Set `trust_remote_code=True` to use a model with custom code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75ac96",
   "metadata": {
    "papermill": {
     "duration": 0.007192,
     "end_time": "2024-02-04T07:33:07.985052",
     "exception": false,
     "start_time": "2024-02-04T07:33:07.977860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6749dbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T07:33:08.000250Z",
     "iopub.status.busy": "2024-02-04T07:33:07.999976Z",
     "iopub.status.idle": "2024-02-04T07:33:30.397236Z",
     "shell.execute_reply": "2024-02-04T07:33:30.396415Z"
    },
    "papermill": {
     "duration": 22.407539,
     "end_time": "2024-02-04T07:33:30.399600",
     "exception": false,
     "start_time": "2024-02-04T07:33:07.992061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 07:33:11.777184: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-04 07:33:11.777290: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-04 07:33:12.059503: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed6b7a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T07:33:30.416089Z",
     "iopub.status.busy": "2024-02-04T07:33:30.415486Z",
     "iopub.status.idle": "2024-02-04T07:33:30.717644Z",
     "shell.execute_reply": "2024-02-04T07:33:30.716754Z"
    },
    "papermill": {
     "duration": 0.312227,
     "end_time": "2024-02-04T07:33:30.719462",
     "exception": false,
     "start_time": "2024-02-04T07:33:30.407235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70051322",
   "metadata": {
    "papermill": {
     "duration": 0.007105,
     "end_time": "2024-02-04T07:33:30.734125",
     "exception": false,
     "start_time": "2024-02-04T07:33:30.727020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"anchor\" id=\"top\" style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto;\n",
    "    padding: 10px;\n",
    "   font-size : 120%;\n",
    "    background-color: #FEF2EF;\n",
    "    border-radius: 2px;\n",
    "    font-color :  #581845  ;        \n",
    "    border: 2px solid #FF5733 ;\"> \n",
    "\n",
    "- Python’s memory allocation and deallocation method is automatic. The user does not have to preallocate or deallocate memory.\n",
    "- Invoking the garbage collector (using `gc.collect`) manually during the execution of a program can be a good idea for how to handle memory being consumed by reference cycles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965972f2",
   "metadata": {
    "papermill": {
     "duration": 0.007019,
     "end_time": "2024-02-04T07:33:30.748560",
     "exception": false,
     "start_time": "2024-02-04T07:33:30.741541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec986f",
   "metadata": {
    "papermill": {
     "duration": 0.007122,
     "end_time": "2024-02-04T07:33:30.762872",
     "exception": false,
     "start_time": "2024-02-04T07:33:30.755750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"anchor\" id=\"top\" style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto;\n",
    "    padding: 10px;\n",
    "   font-size : 120%;\n",
    "    background-color: #FEF2EF;\n",
    "    border-radius: 2px;\n",
    "    font-color :  #581845  ;        \n",
    "    border: 2px solid #FF5733 ;\"> \n",
    "\n",
    "#### Creating a prompt template alongwith the input for passing it to the model\n",
    "- Here I'm passing an abstract to the model, and asking it to create a catchy title that maximises the clickbait. The prompt is in the format as desired by the mistral models (mentioned on [this](https://www.kaggle.com/models/mistral-ai/mixtral/frameworks/PyTorch/variations/8x7b-instruct-v0.1-hf/versions/1) page as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "971679af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T07:33:30.778956Z",
     "iopub.status.busy": "2024-02-04T07:33:30.778239Z",
     "iopub.status.idle": "2024-02-04T07:33:30.785076Z",
     "shell.execute_reply": "2024-02-04T07:33:30.784220Z"
    },
    "papermill": {
     "duration": 0.016954,
     "end_time": "2024-02-04T07:33:30.787168",
     "exception": false,
     "start_time": "2024-02-04T07:33:30.770214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.Write 5 catchy title each not more than 15 words for the following paper abstract. The title should be a single sentence that accurately captures what you have done and sounds interesting to the people who work on the same or a similar topic. The title should contain the important title keywords that other researchers use when looking for literature in databases. The title should also use synonyms, broader terms, or abstractive keywords to make it more appealing and informative. Do not use words that are not related to the paper extract or the topic\n",
      "\n",
      "### Input:Diabetes is now one of the major public health challenges, globally. Prolonged diabetes leads to various diabetic microvascular complications (DMCs) like retinopathy, nephropathy, and neuropathy. Multiple factors are likely to be involved in predisposing diabetic individuals to complications. Early detection or diagnosis is essential in developing strategies to reduce the risk factors and management costs of these diabetic complications. In this study, we employed Raman Spectroscopy (RS) to analyse the plasma samples of diabetes patients without and with DMCs along with the plasma samples of healthy subjects. Spectral comparisons revealed decrease in protein content in Diabetes group and further subsequent decrease in proteins in DMC groups when compared with control group, which corroborates with the fact that there exists increased secretion of proteins in urine and corresponding decreased protein content in their blood in case of diabetic individuals. Among all study groups, it was noted that 75% of control spectra show correct classification, while spectral misclassification is high amongst the subjects with Diabetes and DMCs. Interestingly, very few Diabetes and DMC plasma spectra are misclassified as control spectra. Findings demonstrate that 70% of the Diabetes subjects without complications can be correctly identified from diabetes with complications. Further, investigations could also attempt to explore the use of serum instead of plasma to reduce the spectral misclassifications as one of the abundant constituents namely clotting factors could be avoided. The outcome of RS study may be imminent for the early detection or diagnosis of DMCs.[/INST]\n"
     ]
    }
   ],
   "source": [
    "abstract = 'Diabetes is now one of the major public health challenges, globally. Prolonged diabetes leads to various diabetic microvascular complications (DMCs) like retinopathy, nephropathy, and neuropathy. Multiple factors are likely to be involved in predisposing diabetic individuals to complications. Early detection or diagnosis is essential in developing strategies to reduce the risk factors and management costs of these diabetic complications. In this study, we employed Raman Spectroscopy (RS) to analyse the plasma samples of diabetes patients without and with DMCs along with the plasma samples of healthy subjects. Spectral comparisons revealed decrease in protein content in Diabetes group and further subsequent decrease in proteins in DMC groups when compared with control group, which corroborates with the fact that there exists increased secretion of proteins in urine and corresponding decreased protein content in their blood in case of diabetic individuals. Among all study groups, it was noted that 75% of control spectra show correct classification, while spectral misclassification is high amongst the subjects with Diabetes and DMCs. Interestingly, very few Diabetes and DMC plasma spectra are misclassified as control spectra. Findings demonstrate that 70% of the Diabetes subjects without complications can be correctly identified from diabetes with complications. Further, investigations could also attempt to explore the use of serum instead of plasma to reduce the spectral misclassifications as one of the abundant constituents namely clotting factors could be avoided. The outcome of RS study may be imminent for the early detection or diagnosis of DMCs.'\n",
    "\n",
    "example = {'instruction' : 'Write 5 catchy title each not more than 15 words for the following paper abstract. The title should be a single sentence that accurately captures what you have done and sounds interesting to the people who work on the same or a similar topic. The title should contain the important title keywords that other researchers use when looking for literature in databases. The title should also use synonyms, broader terms, or abstractive keywords to make it more appealing and informative. Do not use words that are not related to the paper extract or the topic',\n",
    "    'input' : abstract }\n",
    "def formatting_func(example):\n",
    "    text = f\"<s>[INST]Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.{example['instruction']}\\n\\n### Input:{example['input']}[/INST]\"\n",
    "    return text\n",
    "eval_prompt = formatting_func(example)\n",
    "print(eval_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42afbb",
   "metadata": {
    "papermill": {
     "duration": 0.00742,
     "end_time": "2024-02-04T07:33:30.801996",
     "exception": false,
     "start_time": "2024-02-04T07:33:30.794576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"anchor\" id=\"top\" style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto;\n",
    "    padding: 10px;\n",
    "   font-size : 120%;\n",
    "    background-color: #FEF2EF;\n",
    "    border-radius: 2px;\n",
    "    font-color :  #581845  ;        \n",
    "    border: 2px solid #FF5733 ;\"> \n",
    "\n",
    "#### To filter out any unnecessary warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d9acedd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T07:33:30.818254Z",
     "iopub.status.busy": "2024-02-04T07:33:30.817699Z",
     "iopub.status.idle": "2024-02-04T07:33:30.822064Z",
     "shell.execute_reply": "2024-02-04T07:33:30.821255Z"
    },
    "papermill": {
     "duration": 0.014458,
     "end_time": "2024-02-04T07:33:30.823983",
     "exception": false,
     "start_time": "2024-02-04T07:33:30.809525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dc76229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T07:33:30.840134Z",
     "iopub.status.busy": "2024-02-04T07:33:30.839595Z",
     "iopub.status.idle": "2024-02-04T07:34:17.390612Z",
     "shell.execute_reply": "2024-02-04T07:34:17.389673Z"
    },
    "papermill": {
     "duration": 46.568289,
     "end_time": "2024-02-04T07:34:17.399632",
     "exception": false,
     "start_time": "2024-02-04T07:33:30.831343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time : 0:00:46.543744\n",
      " 1. Raman Spectroscopy for Early Detection of Diabetic Microvascular Complications: An Analysis of Plasma Samples\n",
      "2. Decreased Protein Content in Diabetes and Diabetic Microvascular Complications: A Raman Spectroscopy Study\n",
      "3. Raman Spectroscopy as a Tool for Predicting Diabetic Microvascular Complications: A Comparative Study of Plasma Samples\n",
      "4. The Role of Raman Spectroscopy in Diagnosing Diabetic Microvascular Complications: A Study of Plasma Samples from Diabetes Patients\n",
      "5. Early Identification of Diabetes Patients at Risk of Microvascular Complications Using Raman Spectroscopy: A Plasma Sample Analysis\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def model_seq_gen(model) : \n",
    "        pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "        start = datetime.now()\n",
    "        sequences = pipe(\n",
    "            f'{eval_prompt}' ,\n",
    "            do_sample=True,\n",
    "            max_new_tokens=200, \n",
    "            temperature=0.7, \n",
    "            top_p=0.95\n",
    "        )\n",
    "        extracted_title = re.sub(r'[\\'\"]', '', sequences[0]['generated_text'].split(\"[/INST]\")[1])\n",
    "        stop = datetime.now()\n",
    "        time_taken = stop-start\n",
    "        print(f\"Execution Time : {time_taken}\")\n",
    "        return extracted_title\n",
    "\n",
    "extracted_title = model_seq_gen(model)\n",
    "print(extracted_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f4124f",
   "metadata": {
    "papermill": {
     "duration": 0.007207,
     "end_time": "2024-02-04T07:34:17.414359",
     "exception": false,
     "start_time": "2024-02-04T07:34:17.407152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"anchor\" id=\"top\" style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto;\n",
    "    padding: 10px;\n",
    "   font-size : 120%;\n",
    "    background-color: #FEF2EF;\n",
    "    border-radius: 2px;\n",
    "    font-color :  #581845  ;        \n",
    "    border: 2px solid #FF5733 ;\"> \n",
    "\n",
    "- `do_sample=True` :  model.generate() method will use Sample Decoding\n",
    "- `max_new_tokens=400` : Number of new tokens you want to generate\n",
    "- `temperature=0.5` :  To increase the probability of probable tokens while reducing the one that is not : <p><img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*41TqaBXrhIGU2V1JCEzU5Q.png\" height=\"600\" width=\"600\" style=\"object-fit: cover;\"></p>\n",
    "<p style=\"text-align:justify;\">\n",
    "</p>   t is the temperature value.At temp=0.5, the most probable words like i, yeah, me, have more chance of being generated. At the same time, this also lowers the probability of the less probable ones, although this does not stop them from occurring.\n",
    "- `top_p=0.95` :  Instead of considering all possible next words, top-p sampling only considers the smallest set of top words whose cumulative probability exceeds a certain threshold, p. A higher value of p means more words are considered, leading to more randomness in the generated text.\n",
    " <p><img src=\"https://api.wandb.ai/files/darek/images/projects/37727390/20e4f024.png\" height=\"700\" width=\"700\" style=\"object-fit: cover;\"></p>\n",
    "<p style=\"text-align:justify;\">\n",
    "\n",
    "Learn more about it [here](https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21074b0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T07:34:17.430434Z",
     "iopub.status.busy": "2024-02-04T07:34:17.430117Z",
     "iopub.status.idle": "2024-02-04T07:34:18.543692Z",
     "shell.execute_reply": "2024-02-04T07:34:18.542521Z"
    },
    "papermill": {
     "duration": 1.124086,
     "end_time": "2024-02-04T07:34:18.545864",
     "exception": false,
     "start_time": "2024-02-04T07:34:17.421778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb  4 07:34:18 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   55C    P0              28W /  70W |  10929MiB / 15360MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   76C    P0              33W /  70W |  13899MiB / 15360MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e6b92",
   "metadata": {
    "papermill": {
     "duration": 0.007619,
     "end_time": "2024-02-04T07:34:18.561444",
     "exception": false,
     "start_time": "2024-02-04T07:34:18.553825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"anchor\" id=\"top\" style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto;\n",
    "    padding: 10px;\n",
    "   font-size : 120%;\n",
    "    background-color: #FEF2EF;\n",
    "    border-radius: 2px;\n",
    "    font-color :  #581845  ;        \n",
    "    border: 2px solid #FF5733 ;\"> \n",
    "\n",
    "- On 2*T4 GPU vram occupied is almost 25 gb/ 30 gb\n",
    "- Inference can be done easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a83abb",
   "metadata": {
    "papermill": {
     "duration": 0.00786,
     "end_time": "2024-02-04T07:34:18.576988",
     "exception": false,
     "start_time": "2024-02-04T07:34:18.569128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"anchor\" id=\"top\" style=\"\n",
    "    margin-right: auto; \n",
    "    margin-left: auto;\n",
    "    padding: 10px;\n",
    "   font-size : 120%;\n",
    "    background-color: #FEF2EF;\n",
    "    border-radius: 2px;\n",
    "    font-color :  #581845  ;        \n",
    "    border: 2px solid #FF5733 ;\"> \n",
    "    \n",
    "    \n",
    "### References : \n",
    "\n",
    "1. https://huggingface.co/docs/transformers/en/main_classes/quantization\n",
    "2. https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "3. https://huggingface.co/docs/transformers/en/custom_models\n",
    "4. https://www.geeksforgeeks.org/garbage-collection-python/\n",
    "5. https://www.kaggle.com/code/sangeek/pynvml-module-to-identify-and-monitor-gpu-usage\n",
    "6. https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc\n",
    "7. https://wandb.ai/darek/llmapps/reports/A-Gentle-Introduction-to-LLM-APIs--Vmlldzo0NjM0MTMz\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 4761,
     "sourceId": 5994,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 997.342115,
   "end_time": "2024-02-04T07:34:22.195319",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-04T07:17:44.853204",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "23816697f88f499abac64f95bd15ef94": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2596e663fbe54737a61996adc05b7c40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4b6cbed3731f49f693f49bdbe0f3ac83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5f8826acdd7d4f40b2b45224a5e20d44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6fc1a7cafd184150a28486395b25cae0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a3d66269cef948f38831df7858b62d8e",
       "placeholder": "​",
       "style": "IPY_MODEL_4b6cbed3731f49f693f49bdbe0f3ac83",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "89fdf3dc32584d2a8783b0987d56c465": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6fc1a7cafd184150a28486395b25cae0",
        "IPY_MODEL_f3164797d2bb4e89a954e917db586069",
        "IPY_MODEL_f068c6f2dfe847b9a5cf60b65cc05cc8"
       ],
       "layout": "IPY_MODEL_fb6b0f1aa18a4251874e3fbcfd8e3034"
      }
     },
     "a3d66269cef948f38831df7858b62d8e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b0b2e798ba7b40fd821e2c1bfe2e318d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f068c6f2dfe847b9a5cf60b65cc05cc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_23816697f88f499abac64f95bd15ef94",
       "placeholder": "​",
       "style": "IPY_MODEL_5f8826acdd7d4f40b2b45224a5e20d44",
       "value": " 19/19 [12:57&lt;00:00, 39.93s/it]"
      }
     },
     "f3164797d2bb4e89a954e917db586069": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2596e663fbe54737a61996adc05b7c40",
       "max": 19.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b0b2e798ba7b40fd821e2c1bfe2e318d",
       "value": 19.0
      }
     },
     "fb6b0f1aa18a4251874e3fbcfd8e3034": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
